{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29965403-abb9-474c-93ba-eb3c616fde70",
   "metadata": {},
   "source": [
    "# Principles of Array Optimization: From Bytecode to Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "11bfd34b-adeb-41ca-ae9d-69d42be696bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import dis\n",
    "import numexpr as ne\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749605f-562a-49e2-a6ea-31f9fe0fd8ae",
   "metadata": {},
   "source": [
    "Introduction goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c5167-86ad-4d3a-89ab-40882790fe78",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d060f2c-aac8-4a9e-bd49-d0706a45b327",
   "metadata": {},
   "source": [
    "What happens in the following code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b0a122-ffb1-4f46-9b78-82812314f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([1, 2, 3])\n",
    "c = np.array([1, 2, 3])\n",
    "d = np.array([1, 2, 3])\n",
    "\n",
    "e = a * b\n",
    "f = e / c\n",
    "g = f**d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a4a63-da17-47bc-9547-2f82539a521f",
   "metadata": {},
   "source": [
    "If you've had some practice with NumPy before, you'll probably know about *Vectorized Array Operations*. These are a NumPy feature which allows NumPy to perform calculations on arrays much more quickly than we could with ordinary lists, not to mention being shorter and easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eb4abd12-cde4-4be1-bb3a-06b04b379021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 582 μs, sys: 735 μs, total: 1.32 ms\n",
      "Wall time: 1.07 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n = 1000\n",
    "\n",
    "a_array= np.random.rand(n) * 100\n",
    "b_array = np.random.rand(n) * 100\n",
    "c_array = np.random.rand(n) * 100\n",
    "d_array = np.random.rand(n)\n",
    "\n",
    "a_list = list(a_array)\n",
    "b_list = list(b_array)\n",
    "c_list = list(c_array)\n",
    "d_list = list(d_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f3aba4c-ef89-4ad5-8f58-154b7344d595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 270 μs, sys: 23 μs, total: 293 μs\n",
      "Wall time: 295 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Using list comprehension\n",
    "\n",
    "e_list = [a*b for a, b, in zip(a_list, b_list)]\n",
    "f_list = [e/c for e, c, in zip(e_list, c_list)]\n",
    "g_list = [f**d for f, d, in zip(f_list, d_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fbfbc421-6087-43b4-a13b-833f1c43a98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 124 μs, sys: 105 μs, total: 229 μs\n",
      "Wall time: 211 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Using vectorized array operations\n",
    "\n",
    "e_array = a_array * b_array\n",
    "f_array = e_array / c_array\n",
    "g_array = f_array**d_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab961db-ff12-4f56-bb3c-e633dc211a5b",
   "metadata": {},
   "source": [
    "Now, looking at this you might wonder why I've split it up into three lines like this. Couldn't I do this all in one line? It'd be shorter, and probably faster too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a3399b2c-44d1-4acc-98de-8659d7936c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_array = ((a_array * b_array)/c_array)**d_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca2861-14e5-4f7a-a1b9-33a3e3dc5c6f",
   "metadata": {},
   "source": [
    "This version is indeed shorter, but it's actually not much faster. In fact, the differences should be almost undetectable.\n",
    "\n",
    "Actually timing Python programs is difficult. There's a lot of unpredictability in how they run, so it's often more informative to look at what these programs produce!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dab665-47b6-4f87-a346-2d6114cfb6ef",
   "metadata": {},
   "source": [
    "## Introducing `dis`assembly\n",
    "\n",
    "You may know that when Python code is run, it's turned into something called **bytecode**. Don't worry if you're not familiar with it, we're going to take a look at a simple example now!\n",
    "\n",
    "The `dis` module, which is built into Python, can be used to 'disassemble' a function and see the bytecode it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d1fab93-6741-4424-8eb5-7b538fad9cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISASSEMBLED MULTI-LINE VERSION\n",
      "\n",
      "  1           0 RESUME                   0\n",
      "\n",
      "  2           2 LOAD_FAST                0 (a_array)\n",
      "              4 LOAD_FAST                1 (b_array)\n",
      "              6 BINARY_OP                5 (*)\n",
      "             10 STORE_FAST               4 (e_array)\n",
      "\n",
      "  3          12 LOAD_FAST                4 (e_array)\n",
      "             14 LOAD_FAST                2 (c_array)\n",
      "             16 BINARY_OP               11 (/)\n",
      "             20 STORE_FAST               5 (f_array)\n",
      "\n",
      "  4          22 LOAD_FAST                5 (f_array)\n",
      "             24 LOAD_FAST                3 (d_array)\n",
      "             26 BINARY_OP                8 (**)\n",
      "             30 STORE_FAST               6 (g_array)\n",
      "\n",
      "  6          32 LOAD_FAST                6 (g_array)\n",
      "             34 RETURN_VALUE\n",
      "\n",
      "DISASSEMBLED SINGLE-LINE VERSION\n",
      "\n",
      "  8           0 RESUME                   0\n",
      "\n",
      "  9           2 LOAD_FAST                0 (a_array)\n",
      "              4 LOAD_FAST                1 (b_array)\n",
      "              6 BINARY_OP                5 (*)\n",
      "             10 LOAD_FAST                2 (c_array)\n",
      "             12 BINARY_OP               11 (/)\n",
      "             16 LOAD_FAST                3 (d_array)\n",
      "             18 BINARY_OP                8 (**)\n",
      "             22 STORE_FAST               4 (g_array)\n",
      "\n",
      " 11          24 LOAD_FAST                4 (g_array)\n",
      "             26 RETURN_VALUE\n"
     ]
    }
   ],
   "source": [
    "def foo(a_array, b_array, c_array, d_array):\n",
    "    e_array = a_array * b_array\n",
    "    f_array = e_array / c_array\n",
    "    g_array = f_array**d_array\n",
    "    \n",
    "    return g_array\n",
    "\n",
    "def bar(a_array, b_array, c_array, d_array):    \n",
    "    g_array = ((a_array * b_array)/c_array)**d_array\n",
    "    \n",
    "    return g_array\n",
    "\n",
    "print(\"DISASSEMBLED MULTI-LINE VERSION\\n\")\n",
    "dis.dis(foo)\n",
    "\n",
    "print(\"\\nDISASSEMBLED SINGLE-LINE VERSION\\n\")\n",
    "dis.dis(bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcedeff-4115-44e7-8735-0791aba18491",
   "metadata": {},
   "source": [
    "This is what Python code looks like under the hood!\n",
    "\n",
    "Each line is a separate instruction telling Python what to do: `LOAD_FAST` and `STORE_FAST` are storing the addresses and names of the arrays, while all the math is done in the `BINARY_OP` step. \n",
    "\n",
    "The single-line version is shorter, but only by a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7b1a8e81-e358-4977-9137-0ab980282679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in the instructions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'LOAD_FAST e_array',\n",
       " 'LOAD_FAST f_array',\n",
       " 'STORE_FAST e_array',\n",
       " 'STORE_FAST f_array'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo_set = set({\n",
    "    \"LOAD_FAST a_array\",\n",
    "    \"LOAD_FAST b_array\",\n",
    "    \"BINARY_OP *\",\n",
    "    \"STORE_FAST e_array\",\n",
    "\n",
    "    \"LOAD_FAST e_array\", \n",
    "    \"LOAD_FAST c_array\",\n",
    "    \"BINARY_OP /\",\n",
    "    \"STORE_FAST f_array\",\n",
    "\n",
    "    \"LOAD_FAST f_array\",\n",
    "    \"LOAD_FAST d_array\",\n",
    "    \"BINARY_OP **\",\n",
    "    \"STORE_FAST g_array\",\n",
    "})\n",
    "\n",
    "bar_set = set({\n",
    "    \"LOAD_FAST a_array\",\n",
    "    \"LOAD_FAST b_array\",\n",
    "    \"BINARY_OP *\",\n",
    "    \"LOAD_FAST c_array\",\n",
    "    \"BINARY_OP /\",\n",
    "    \"LOAD_FAST d_array\",\n",
    "    \"BINARY_OP **\",\n",
    "    \"STORE_FAST g_array\",\n",
    "})\n",
    "\n",
    "print(\"Difference in the instructions:\")\n",
    "foo_set - bar_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb22676-a11a-4022-aedf-a542deb85da4",
   "metadata": {},
   "source": [
    "The only difference here is that we're storing arrays e and f, which we do extremely quickly. All the hard work in `BINARY_OP` is shared.\n",
    "\n",
    "In the multi-line version, the intermediary products of this array operation have names and remain available for us to examine. In the single line version, they're not given names and are quickly deleted. But we did create them!\n",
    "\n",
    "Those of you with more experience in computer science may have already realized what this means: every single `BINARY_OP` allocates a new array onto the heap. \n",
    "\n",
    "If you're not familiar with the concepts of stack and heap, this summary will suffice for the moment: your computer has different regions of memory used for different kinds of work, and the two most common types are stack and heap. The stack is generally smaller, and working with it is extremely fast, but it's more restrictive. The heap has much more capacity, is slower, but can be used more freely. The stack's speed comes from its restrictions. \n",
    "\n",
    "Allocating new memory on the heap is one of the slower things modern computers do. Our judgment of speed is very relative. 'Slow' here is still very fast by human standards: it takes a few millionths of a second. But other things, like simple math, computers can do in **billionths of a second**.\n",
    "\n",
    "One of the most common problems when optimizing code is making sure that the fast operations - like the math - aren't waiting ages and ages on something else - like memory allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb46293e-e020-4013-b814-4e59fe212cf6",
   "metadata": {},
   "source": [
    "## Exploring `BINARY_OP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e2e51-3b63-44c6-aa12-52ba6dbde71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e70f47f4-f24c-4364-a7e2-391cf8351d66",
   "metadata": {},
   "source": [
    "## But what is an `np.ndarray`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34230f33-9277-4cf2-bb36-2e00df7a0cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81102683-f93d-4d55-b999-902af997fa21",
   "metadata": {},
   "source": [
    "## Eliminating Intermediary Allocations\n",
    "\n",
    "Despite all the work that `BINARY_OP` does, the allocation of new arrays is the most significant outside of actually doing the math. \n",
    "\n",
    "NumPy has to do this because of how it's designed (there are good reasons for this, which we don't have time to explore here), but we don't actually need it: we only care about the final output. Is there a way to do this while only allocating one array?\n",
    "\n",
    "In theory, it shouldn't be hard. We could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f1645523-cee8-4a0a-af62-ad34012dec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.4 ms, sys: 17 μs, total: 1.42 ms\n",
      "Wall time: 1.41 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "out_array = np.zeros(n)\n",
    "\n",
    "for i, (a_scalar, b_scalar, c_scalar, d_scalar), in enumerate(zip(a_array, b_array, c_array, d_array)):\n",
    "    out_array[i] = ((a_scalar*b_scalar)/c_scalar)**d_scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc0256-9711-4d88-81d3-b3bb7a291602",
   "metadata": {},
   "source": [
    "We can define all our arrays, then allocate just one new array (defaulting to all zeros). Then we can iterate through all the arrays at the same time, pulling out the values one at a time, and doing the math on the scalars. This should work, right?\n",
    "\n",
    "---\n",
    "\n",
    "Well, yes, it will work. But it will also be **very slow**. Even slower than using list comprehension, despite the fact that we're iterating through arrays!\n",
    "\n",
    "What's happening under the hood here?\n",
    "\n",
    "Every single iteration, we have to extract the elements: these are coming from contiguous arrays, so we benefit from caching here, but each C float needs to be wrapped in an `np.float64` object, and then allocated onto the heap.\n",
    "\n",
    "Yes, CS student, I see you. Yes, known-size scalar numeric types get allocated on the heap, not the stack. Welcome to Python.\n",
    "\n",
    "Then we have to do all the same binary operation setup we saw before, but *on each element* rather than once per array, because Python doesn't know or care that these values originally came from a NumPy array: we're firmly in native-Python loop territory here, which means all of NumPy's nice guarantees are void and its optimizations are inaccessible.\n",
    "\n",
    "Then, once we've actually gone ahead and done the math, producing the output, *that* needs to be transformed into a heap-allocated object too before being inserted to the array, where the vectorized operation could do this without the *per-element* intermediary allocation.\n",
    "\n",
    "We've traded a handful of up-front allocations on a per-array basis with a lot more tiny allocations on a per-element basis. Since allocation time does **not** scale linearly with the size of the memory being allocated, this is a bad trade.\n",
    "\n",
    "Oh, and there's also Python loop overhead that doesn't exist for the vectorized operations, which are also capable of SIMD optimizations that the native Python loop is a dozen layers away from being able to perform. We'll get into these details later, but just so you know.\n",
    "\n",
    "---\n",
    "\n",
    "Unfortunately, the easy, straightforward approach isn't viable. What other approaches can we take to speed this up?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32594b66-4a9a-42f1-bede-baf536a8fcee",
   "metadata": {},
   "source": [
    "## Approach 1: `NumExpr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b88b07-48c0-4056-8154-08aee8b98ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1349033d-9003-4ae3-8a9d-2c72909bb6da",
   "metadata": {},
   "source": [
    "## Approach 2: JIT Compilation: `Numba`/`JAX`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "017f3d4e-a371-4de0-8fd5-d1fbdc8a66cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.78 ms, sys: 2.25 ms, total: 5.03 ms\n",
      "Wall time: 3.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "a_jarray = jnp.array(a_array)\n",
    "b_jarray = jnp.array(b_array)\n",
    "c_jarray = jnp.array(c_array)\n",
    "d_jarray = jnp.array(d_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f17daec-f07f-4f37-b963-86f529c8c699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.65 ms, sys: 1.2 ms, total: 2.85 ms\n",
      "Wall time: 1.56 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "g_jarray = ((a_jarray * b_jarray)/c_jarray)**d_jarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "903e3b15-711b-48f5-8494-f7545b87a34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 200 μs, sys: 24 μs, total: 224 μs\n",
      "Wall time: 224 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def bar(a_array, b_array, c_array, d_array):\n",
    "    g_array = ((a_array * b_array)/c_array)**d_array\n",
    "    return g_array\n",
    "\n",
    "bar_compiled = jit(bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7bb9e9c7-234f-4bcc-8c4a-aedb3c34cece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 775 μs, sys: 341 μs, total: 1.12 ms\n",
      "Wall time: 662 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "res = bar_compiled(a_jarray, b_jarray, c_jarray, d_jarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485ea7ed-f16f-4177-95dc-ffae6366de21",
   "metadata": {},
   "source": [
    "## Approach 3: Low-level solutions\n",
    "\n",
    "Sometimes, the best solution really is to leave Python and drop down to a lower-level language.\n",
    "\n",
    "It's quite common, especially when writing code for high-performance libraries, to write code in a faster, compiled langauge and then let people call that code from Python. NumPy, NumExpr, JAX, and pretty much all high-performance numeric libraries have a core written in another language that's optimized for speed, and then a Python library is built around it.\n",
    "\n",
    "The traditional choice for writing these libraries is the C programming language, or more rarely C++: however, these days more and more people are using Rust, because it is both fast and safe, and has a lot of nice, modern affordances. We'll go through an example of Rust-Python code here.\n",
    "\n",
    "Note that in Rust, comments are written with `//`, not `#` like in Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af02fb1-2f2c-4e9f-8248-a1feccb0faab",
   "metadata": {},
   "source": [
    "```rust\n",
    "// this is Rust's equivalent of Python's `import`\n",
    "// we're importing pyo3, which makes writing Python+Rust code much easier,\n",
    "// and also importing Rust's version of NumPy, which allows us to use NumPy functions from Rust\n",
    "use pyo3::prelude::*;\n",
    "use numpy::{PyArray1, PyReadonlyArray1, PyArrayMethods};\n",
    "\n",
    "// this `pyfunction` decorator tells Rust that this function is meant to be called from Python\n",
    "#[pyfunction]\n",
    "// in Rust, the arguments of a function are inside parentheses `()` like in Python, the \n",
    "// type of the value being returned is after an arrow `->` and the body of the function is \n",
    "// inside braces `{}`\n",
    "//\n",
    "// ```\n",
    "// fn foo() {\n",
    "//    ...\n",
    "// } \n",
    "// ```\n",
    "// is the Rust equivalent of \n",
    "//\n",
    "// ```\n",
    "// def foo():\n",
    "//     ...\n",
    "// ```\n",
    "//\n",
    "// this defines a public function named `foo` that has access to the `'py` lifetime.\n",
    "// 'public' just means the function can be exported to be used in other places.\n",
    "// Lifetimes are an advanced Rust concept which we don't need to know about here. You\n",
    "// can learn more about them at this link: https://doc.rust-lang.org/rust-by-example/scope/lifetime.html\n",
    "pub fn foo<'py>(\n",
    "    // we take Python itself as an argument. How this works is a bit complicated,\n",
    "    // but it means this function will only run if Python is running. \n",
    "    // See the documentation here: https://docs.rs/pyo3/latest/pyo3/marker/struct.Python.html\n",
    "    py: Python<'py>,\n",
    "    // we take several NumPy arrays as arguments: in Rust, you *have* to put\n",
    "    // the type of each argument after its name, and only one type is allowed!\n",
    "    // this argument means 'a NumPy array of 64-bit floats that I promise not\n",
    "    // to modify'\n",
    "    a_arr: PyReadonlyArray1<f64>,\n",
    "    b_arr: PyReadonlyArray1<f64>,\n",
    "    c_arr: PyReadonlyArray1<f64>,\n",
    "    d_arr: PyReadonlyArray1<f64>,\n",
    "\n",
    "// this tells us what the function returns. You might have seen something similar in Python,\n",
    "// but in Rust, the return signature is mandatory. This type signature means we're returning \n",
    "// one 1-d numpy array of 64-bit floats (equivalent to numpy's np.float64 type), and we're \n",
    "// giving it back to Python\n",
    ") -> Bound<'py, PyArray1<f64>> {\n",
    "\n",
    "    // take arrays as read-only slices\n",
    "    let a_slice = a_arr.as_slice().unwrap();\n",
    "    let b_slice = b_arr.as_slice().unwrap();\n",
    "    let c_slice = c_arr.as_slice().unwrap();\n",
    "    let d_slice = d_arr.as_slice().unwrap();\n",
    "\n",
    "    // create a new NumPy array of the same length as the ones we took as arguments...\n",
    "    let out_arr = unsafe { PyArray1::new(py, a_slice.len(), false)};\n",
    "    // and take it as a mutable slice\n",
    "    let out_slice = unsafe { out_arr.as_slice_mut().unwrap() };\n",
    "\n",
    "    // both of the operations above are `unsafe`! This means that the computer cannot prove\n",
    "    // they are safe, but we as programmers are telling it to chill out, we are\n",
    "    // guaranteeing safety manually and taking responsibility in exchange for ignoring rules\n",
    "\n",
    "    // our dumb loop goes through element by element, fusing the \n",
    "    // math operations into a single step without intermediate allocations\n",
    "    // this is the same thing we tried to do earlier in Python!\n",
    "\n",
    "    // Rust's syntax is messier, but this is the same as Python's\n",
    "    // ```\n",
    "    // for i, (a, b, c, d), in enumerate(zip(a_array, b_array, c_array, d_array)):\n",
    "    // ```\n",
    "    // we're zipping the arrays together, iterating through them, and taking\n",
    "    // using one element from each at a time\n",
    "    for (i, (((a, b), c), d)) in a_slice.iter()\n",
    "        .zip(b_slice)\n",
    "        .zip(c_slice)\n",
    "        .zip(d_slice)\n",
    "        .enumerate() {\n",
    "\n",
    "        // this is just ((a*b)/c)**d: we do the math and load the result \n",
    "        // directly into the ith position in the out_slice\n",
    "        out_slice[i] = ((a*b)/c).powf(*d);\n",
    "    }\n",
    "    // return the out array to Python - doesn't require a `return` keyword if it's \n",
    "    // the last thing in the function\n",
    "    out_arr\n",
    "\n",
    "    // we're done!\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a498918-c374-46e4-a85b-304faf138bad",
   "metadata": {},
   "source": [
    "That was a lot! Despite how long this is, we've actually done quite little: unlike Python's operators or `BINARY_OP` instruction, which look simple on the surface but hide layers upon layers of checks, allocations, pointer chasing, and method calls, the code above is fairly low-level and a lot closer to 'what you see is what you get' (though even here, there are plenty of abstractions above machine code).\n",
    "\n",
    "A lot of the code above, such as the `.as_slice().unwrap()`s, is just about type manipulation: it doesn't cause anything to actually *happen* when the code runs, but it proves that the code is safe *before* it runs.\n",
    "\n",
    "In the end, what this does is quite simple: we get pointers to the start of these arrays, figure out how long they are, allocate a new array to hold the results (just one, no intermediate allocations) and then step through element by element, doing the calculation and writing the results to the existing array. \n",
    "\n",
    "If that sounds familiar, it's because that's exactly the kind of dumb-as-rocks loop we tried to build at the start of this notebook. The difference is that, now, it actually works as intended: there's no extra loop overhead, no allocation of scalar values, it's all happening on the stack. \n",
    "\n",
    "But don't take my word for it! Let's see the proof directly by learning to review x86 assembly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f414878-78c5-433b-9fe8-9f69d95fcd16",
   "metadata": {},
   "source": [
    "## What is Assembly?\n",
    "\n",
    "Assembly is the generic term for an instruction set that maps directly to electronic hardware. \n",
    "\n",
    "You're probably aware that, at the very bottom level, all data and software is represented by binary: strings of 1s and 0s. You may also have heard of logic gates: the physical systems inside electronic circuitry that modify binary. \n",
    "\n",
    "Assembly exists a level above that: assembly language commands a microchip to perform certain distinct, hardware-level actions, working directly with registers. Don't worry if you're not familiar with these, we'll cover the basics slowly.\n",
    "\n",
    "In the modern day, it's quite rare for software engineers to directly write or even read assembly language - most don't know it at all and don't encounter after university. Even so, it remains very useful to be able to read assembly, in order to understand what a computer is actually doing. \n",
    "\n",
    "Rather than being written by hand, modern assembly is generated by compilers: programs that turn code into assembly. \n",
    "\n",
    "'Low-level' languages C and Rust compile directly to assembly... but even here, there's an intermediate step. Compilers generally turn written code into an intermediate representation (IR), which is then processed into assembly. \n",
    "\n",
    "What about 'higher level' languages like Python? Python (or at least, the most common Python engine) is written in C: this is called the [CPython API](https://docs.python.org/3/c-api/index.html). \n",
    "\n",
    "The bytecode we saw before is even higher-level: it coordinates calls to the CPython API. A bytecode instruction like BINARY_OP triggers a whole cascade of CPython API calls, which eventually bottom out in a series of assembly instructions, which direct the activity of logic gates burned into the hardware.\n",
    "\n",
    "Lower <- Higher\n",
    "\n",
    "Binary <- logic gates <- assembly <- LLVM IR <- C / Rust <- CPython API <- Bytecode <- Python\n",
    "\n",
    "We've used the term 'low-level' before, but this term is very relative. Back in the 1970s, C was considered *high-level*! \n",
    "\n",
    "The reason going lower-level tends to lead to increased performance is that your intent gets more specific and there are fewer abstractions between your intent and the hardware. Fewer things that need to be checked at runtime, fewer layers of API calls. But the same code is less flexible and must be more verbose to communicate the same intent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6034b4-a4e9-4ffc-b1bd-9ec4c72aadfa",
   "metadata": {},
   "source": [
    "## Assembly Code Crash Course\n",
    "\n",
    "Assembly is read line-by-line. Each line begins with a command, like `mov`, `call`, `add`, etc. Each command is followed by either one or two arguments. These arguments are either locations in memory described by a pointer `ptr`, or registers. \n",
    "\n",
    "Registers are special regions of memory that exist directly inside a microchip. This allows processors to access them extremely quickly and work with them directly. But they are also tiny. The largest registers can hold just 512 bits. \n",
    "\n",
    "The size of a register is described by the first letter of its name:\n",
    "\n",
    "- `a`: 16-bit\n",
    "- `e`: 32-bit\n",
    "- `r`: 64-bit\n",
    "- `xmm`: 128-bit (special purpose, introduced by SSE)\n",
    "- `ymm`: 256-bit (special purpose, introduced by AVX)\n",
    "- `zmm`: 512-bit (special purpose, introduced by AVX-512)\n",
    "\n",
    "x86 assembly also describes memory sizes using 'words'.\n",
    "\n",
    "- `word`: 16-bit\n",
    "- `dword` or 'double-word': 32-bit\n",
    "- `qword` or 'quad-word': 64-bit\n",
    "- `xmmword`: 128-bit\n",
    "- `ymmword`: 256-bit\n",
    "- `zmmword`: 512-bit\n",
    "\n",
    "Why are the word prefixes the same for some registers and not others? Legacy compatibility. Nobody designed it this way from the start. \n",
    "\n",
    "When there are two arguments after a command, they typically follow the format `command destination, source`. For example, if you have 64-bit numbers loaded into registers `r1` and `r2`, and you want to add them, you would use:\n",
    "\n",
    "```\n",
    "add     r1, r2\n",
    "```\n",
    "\n",
    "This will add the contents of `r1` and `r2`, and save the result to `r1`. The previous contents of `r1` are lost, unless deliberately backed up elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27db41-37d9-4f8d-99bb-1fd25c9803a2",
   "metadata": {},
   "source": [
    "## An Example of Assembly Code\n",
    "\n",
    "Here's a modified version of the Python+Rust code we looked at before. This code is Rust-only, since [Compiler Explorer](https://godbolt.org) doesn't support `pyo3`.\n",
    "\n",
    "```rust\n",
    "pub fn foo(\n",
    "    // `&[f64]` is a reference to an array of 64-bit floats\n",
    "    a_arr: &[f64],\n",
    "    b_arr: &[f64],\n",
    "    c_arr: &[f64],\n",
    "    d_arr: &[f64],\n",
    "// the return type `Vec<f64>` is an owned array of 64-bit floats\n",
    ") -> Vec<f64> {\n",
    "\n",
    "    // note that we don't need to take a mutable slice here, because all these \n",
    "    // arrays are entirely in Rust-controlled memory.\n",
    "    let mut out = vec![0.0; a_arr.len()];\n",
    "\n",
    "    // otherwise, it's exactly the same!\n",
    "    for (i, (((a, b), c), d)) in a_arr.iter()\n",
    "        .zip(b_arr)\n",
    "        .zip(c_arr)\n",
    "        .zip(d_arr)\n",
    "        .enumerate() {\n",
    "\n",
    "        out[i] = ((a*b)/c).powf(*d);\n",
    "    }\n",
    "    out\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "With default settings, this function compiles to ~1700 lines of assembly. If we instead select the most aggressive optimization settings by passing the `-O` flag, it comes down to just over 150 lines!\n",
    "\n",
    "But the part we care about is the vectorized loop, which is even shorter. Here is is in its entirety:\n",
    "\n",
    "```\n",
    "movupd  xmm1, xmmword ptr [r13 + r15]\n",
    "movupd  xmm0, xmmword ptr [rbx + r15]\n",
    "mulpd   xmm0, xmm1\n",
    "movupd  xmm1, xmmword ptr [rcx + r15]\n",
    "divpd   xmm0, xmm1\n",
    "movapd  xmmword ptr [rsp + 80], xmm0\n",
    "movups  xmm1, xmmword ptr [rdx + r15]\n",
    "movaps  xmmword ptr [rsp + 96], xmm1\n",
    "mov     r12, rbx\n",
    "mov     rbx, rdx\n",
    "call    rbp\n",
    "movapd  xmmword ptr [rsp + 32], xmm0\n",
    "movapd  xmm0, xmmword ptr [rsp + 80]\n",
    "unpckhpd        xmm0, xmm0\n",
    "movaps  xmm1, xmmword ptr [rsp + 96]\n",
    "movhlps xmm1, xmm1\n",
    "call    rbp\n",
    "mov     rdx, rbx\n",
    "mov     rbx, r12\n",
    "mov     rcx, qword ptr [rsp + 24]\n",
    "mov     rax, qword ptr [rsp]\n",
    "movapd  xmm1, xmmword ptr [rsp + 32]\n",
    "unpcklpd        xmm1, xmm0\n",
    "movupd  xmmword ptr [rax + r15], xmm1\n",
    "add     r15, 16\n",
    "cmp     r14, r15\n",
    "jne     .LBB0_9\n",
    "mov     rsi, qword ptr [rsp + 56]\n",
    "mov     rdi, qword ptr [rsp + 16]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f78a74-3fd7-48b5-a099-a8a9079be63a",
   "metadata": {},
   "source": [
    "## Breaking Down A Vectorized Loop in Assembly\n",
    "\n",
    "Let's take this apart.\n",
    "\n",
    "```\n",
    "movupd  xmm1, xmmword ptr [r13 + r15]   ; load 2 doubles from array A\n",
    "movupd  xmm0, xmmword ptr [rbx + r15]   ; load 2 doubles from array B\n",
    "mulpd   xmm0, xmm1                      ; xmm0 = a * b (packed)\n",
    "movupd  xmm1, xmmword ptr [rcx + r15]   ; load 2 doubles from array C\n",
    "divpd   xmm0, xmm1                      ; xmm0 /= c (packed)\n",
    "```\n",
    "\n",
    "First, we load an `xmmword` from a memory location `ptr [r13 + r15]` into the `xmm1` register. Recall than `xmm` is 128 bits, so `xmmword ptr [r13 + r15]` means \"the first 128 bits starting from the position `[r13+r15]`.\"\n",
    "\n",
    "In terms of our program, this is loading the first two 64-bit numbers from array A into a 128-bit register.\n",
    "\n",
    "We then do the same from `[rbx + r15]`, which is where we find of array B, loading the first two b values into the `xmm0` register.\n",
    "\n",
    "The command used here is `movupd` (**mov**e **u**naligned **p**acked **d**oubles). A 'double' is another word for a 64-bit float, short for 'double-precision floating point': 32-bit floats are 'singles'. \n",
    "\n",
    "'Unaligned' in this context means that the data isn't guaranteed to start at an 8-byte boundary. We won't cover byte alignment, but you can find more information [here](https://en.wikipedia.org/wiki/Data_structure_alignment). 'Packed' means that they're smushed next to each other and being moved around as one unit. \n",
    "\n",
    "Once our As and Bs are in the registers, we can multiply them with `mulpd` (**mul**tiply **p**acked **d**oubles). This uses a single multiplication instruction to perform two multiplications, and store their results in xmm0. For a comparison to Python, imagine you had two tuples and multiplied them like so:\n",
    "\n",
    "```python\n",
    "a = (1.0, 3.5)\n",
    "b = (8.0, 2.0)\n",
    "\n",
    "b *= a\n",
    "\n",
    "assert(b == (8.0, 7.0))\n",
    "```\n",
    "\n",
    "This doesn't actually work in Python, becaue tuples don't allow element-wise arithmetic (unless you use NumPy) but that's what's happening here!\n",
    "\n",
    "We use the terminology 'lower' and 'upper' to refer to the first 64 and last 64 bits, respectively, in each register. \n",
    "\n",
    "What happens next?\n",
    "\n",
    "We load two more doubles from *yet another* memory address into xmm1, this time corresponding to array C, and use `divpd` (**div**ide **p**acked **d**oubles), storing the result in xmm0, exactly the same way we did the multiplication.\n",
    "\n",
    "This means that our loop is completing `(a*b)/c` in just five assembly instructions total... and that's for two elements at once, not one! This is the power of vectorized math operations: the same calculations, right at the assembly level, can be done in batches, making them twice as fast. \n",
    "\n",
    "This is obviously way better than our attempt at the same dumb loop in Python, since we're not performing so many implicit allocations each time, but it's also superior to NumPy's vectorized array operations on the same hardware: NumPy is also using vectorized instructions like `mulpd`, but it's doing all the multiplications, then a whole heap allocation, then all the divisions, while this implementation fuses the multiplication and division into the same iteration step.\n",
    "\n",
    "---\n",
    "\n",
    "Next, we have to perform the `**d` operation. This ends up being a good deal more complicated.\n",
    "\n",
    "Elsewhere in the assembly code, we did this:\n",
    "\n",
    "```\n",
    "mov     rbp, qword ptr [rip + pow@GOTPCREL]\n",
    "```\n",
    "\n",
    "This **mov**es the 64-bit contents of `[rip + pow@GOTPCREL]` into the `rbp` register, which is typically used for 64-bit stack pointers. \n",
    "\n",
    "What in the world is `[rip + pow@GOTPCREL]`? It's Rust's built-in function for power operations! This will be important shortly.\n",
    "\n",
    "```\n",
    "movapd  xmmword ptr [rsp + 80], xmm0    ; xmm0 to local stack (16 bytes)\n",
    "movups  xmm1, xmmword ptr [rdx + r15]   ; load 2 doubles from D\n",
    "movaps  xmmword ptr [rsp + 96], xmm1    ; ... and then put them on the local stack, bc x86 doesn't allow mem-to-mem moves\n",
    "mov     r12, rbx                        ; a little more juggling to save into callee-managed registers\n",
    "mov     rbx, rdx\n",
    "```\n",
    "\n",
    "We now move use `movapd` (**mov**e *a*ligned **p**acked **d**oubles) to move the contents of `xmm0` (which right now contain the results of `(a*c)/c` for two elements) onto the stack. We do this to 'back them up' so to speak. \n",
    "\n",
    "We then load two doubles from array D into `xmm1`... and immediately move them onto the stack as well. This takes two instructions because x86 architecture does not allow us to move things from one point in memory to another point in memory: you have to put them in a register first. \n",
    "\n",
    "You'll notice we use a slightly different command, `movaps` (**mov**e *a*ligned **p**acked **s**ingles) instead of the doubles version. There are some cases where this would make a difference, but here it doesn't, and the compiler's choice to use one or the other is arbitrary.\n",
    "\n",
    "We then move things around a bit more (I'm fond of calling this 'register juggling'), and we're finally ready for the main event:\n",
    "\n",
    "```\n",
    "call    rbp                             ; xmm0[0] ** xmm1[0]\n",
    "```\n",
    "\n",
    "The `call` command calls a function by taking a pointer to that function's location. Right now, the `rbp` register contains a power function, so `call    rbp` means 'raise the values in `xmm0` to the power of the values in `xmm1`'. \n",
    "\n",
    "You might ask \"didn't we move the values of D out of `xmm1`? Shouldn't it be empty?\" It's not, because the `mov` command and its variants do not destroy the original data - functionally, you should think of it as copying data, rather than moving it.\n",
    "\n",
    "But there's a catch: Rust's power function works on just one set of 64-bit floats at a time. When we try to call it on a larger register, what does it do? It just uses the first 64 bits (aka the low bits) and ignores the rest. So what's happening here is more like\n",
    "\n",
    "```python\n",
    "xmm0 = (base1, base2)\n",
    "xmm1 = (exponent1, exponent2)\n",
    "\n",
    "xmm0 **= xmm1\n",
    "\n",
    "assert(xmm0 = (result1, base2))\n",
    "```\n",
    "\n",
    "The Rust compiler doesn't have a built-in function for exponentiating packed doubles, and neither does x86 assembly. If we want to finish the job, we have to do some more juggling.\n",
    "\n",
    "```\n",
    "movapd  xmmword ptr [rsp + 32], xmm0    ; store result[0] on stack\n",
    "movapd  xmm0, xmmword ptr [rsp + 80]    ; move previous (a*b)c/ back from stack\n",
    "unpckhpd        xmm0, xmm0              ; shift higher bits to lower position\n",
    "movaps  xmm1, xmmword ptr [rsp + 96]    ; do the same for xmm1\n",
    "movhlps xmm1, xmm1\n",
    "call    rbp                             ; xmm0[1] ** xmm1[1]\n",
    "```\n",
    "\n",
    "We're going to store the first result on the stack to avoid losing it. Then we're going to take the previous output we stored on the stack, which contains the lower and upper bits of `(a*b)/c`, and move it back into xmm0. \n",
    "\n",
    "Then we're going to use a neat trick: `unpckhpd` (**unp**a**ck** **h**igh **p**acked **d**oubles) takes the top 64 bits of xmm0, and *moves them down to replace the lower bits*. We do the same thing for `xmm1`, loading the previously stored values from array D, moving the high bits down, and using `call    rbp` again. \n",
    "\n",
    "And that's the important part! The rest of the assembly stores the results properly, inserts them into the output array, and sets up the next loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e5cfb-89c3-46f7-9d78-a49b91483709",
   "metadata": {},
   "source": [
    "## Lessons and Benchmarking\n",
    "\n",
    "That was a lot, especially if you haven't worked with assembly before. Take a break, drink some coffee or tea, and come back to this with some rest. From here, we're not writing more code (in any language), we're evaluating the results of the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1a8fc-bd33-476b-ab3e-ae06fce76a1b",
   "metadata": {},
   "source": [
    "### Bytecode vs Assembly\n",
    "\n",
    "First, while it's still fresh in your mind, I want to compare the assembly we just examined with the bytecode we saw at the very beginning:\n",
    "\n",
    "```\n",
    "  2 LOAD_FAST                0 (a_array)\n",
    "  4 LOAD_FAST                1 (b_array)\n",
    "  6 BINARY_OP                5 (*)\n",
    " 10 LOAD_FAST                2 (c_array)\n",
    " 12 BINARY_OP               11 (/)\n",
    " 16 LOAD_FAST                3 (d_array)\n",
    " 18 BINARY_OP                8 (**)\n",
    " 22 STORE_FAST               4 (g_array)\n",
    "```\n",
    "\n",
    "Load, load, op, load, op, load, op, return.\n",
    "\n",
    "Conceptually, it's not so different from the assembly! After all, bytecode is itself a stack-based instruction set for orchestrating the \n",
    "\n",
    "But the assembly reflects commands to the hardware, each taking fractions of a nanosecond to complete and existing entirely on the stack, while the bytecode exists many layers of abstraction above the hardware, and each command hides multiple levels of pointer indirection, heap allocations, and type checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2003f17b-6fc1-4ed1-9a95-c1cfdb72e892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb1534-a4d5-4691-b6a0-1423f5cc11e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b151648-bb39-41d3-833e-3319398e63a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
